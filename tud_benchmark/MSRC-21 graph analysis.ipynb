{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0156d35e",
   "metadata": {},
   "source": [
    "# Node embeddings and the MSRC-21 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f739ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-28 17:24:59.567121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import plot_functions\n",
    "import data_processing\n",
    "import os\n",
    "import numpy as np\n",
    "import networkit\n",
    "from auxiliarymethods import datasets as dp\n",
    "from auxiliarymethods.reader import tud_to_networkx\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import KernelPCA, TruncatedSVD\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from scipy.sparse import load_npz\n",
    "import auxiliarymethods.auxiliary_methods as aux\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import deepcopy\n",
    "import IPython\n",
    "import re\n",
    "import ipympl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ccef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def load_csv(path):\n",
    "    return np.loadtxt(path, delimiter=\";\")\n",
    "\n",
    "def load_sparse(path):\n",
    "    return load_npz(path)\n",
    "\n",
    "def select_from_list(l, indices):\n",
    "    return [l[i] for i in indices]\n",
    "\n",
    "def cluster_average_nodes(G,classes):\n",
    "    nodes_per_class = dict.fromkeys(np.unique(classes),0)\n",
    "    graphs_per_class = dict.fromkeys(np.unique(classes),0)\n",
    "    edges_per_class = dict.fromkeys(np.unique(classes),0)\n",
    "    for graph in G:\n",
    "        cluster = graph.graph[\"classes\"][0]\n",
    "        nodes_per_class[cluster] += len(graph.nodes)\n",
    "        graphs_per_class[cluster] += 1\n",
    "        edges_per_class[cluster] += len(graph.edges)\n",
    "\n",
    "    average_nodes = {}\n",
    "    average_edges = {}\n",
    "    for cluster,no_graphs in graphs_per_class.items():\n",
    "        average_nodes[cluster] = nodes_per_class[cluster] / no_graphs\n",
    "        average_edges[cluster] = edges_per_class[cluster] / no_graphs\n",
    "    return average_nodes,average_edges,graphs_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e29d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in data set is 563\n",
      "Number of classes 20\n"
     ]
    }
   ],
   "source": [
    "# loading and preparing data\n",
    "\n",
    "base_path = os.path.join(\"kernels\", \"node_labels\")\n",
    "ds_name = \"MSRC_21\"\n",
    "classes = dp.get_dataset(ds_name)\n",
    "G = tud_to_networkx(ds_name)\n",
    "print(f\"Number of graphs in data set is {len(G)}\")\n",
    "print(f\"Number of classes {len(set(classes.tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_classes = data_processing.create_img_index_by_class(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de0e53",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "\n",
    "## The MSRC-21 dataset\n",
    "\n",
    "The idea behind using graphs kernels on images is that an image can be described as consisting of connected superpixels. Superpixels summarise the information of a part of the picture and form nodes in the image graph, with adjacent superpixels being connected by nodes. The MSRC 21 dataset is a standard benchmark dataset for this kind of analysis first published by Winn et al. 2005 as part of Microsoft's research efforts. The image graph consists of image segments which carry a semantic label (see figure below). These semantic labels may both fall into a general \"void\" category and may have no label at all, representing noise. The MSRC-21 dataset contains the ground-truth of its image segmentation and is often used to test the accuracy of image segmentation algorithms; in our case, the main goal is the general classification of the images by theme given the known a semantically labelled graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ae9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(\"assets/Neumann et al 2015 Fig 4.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2b4ee",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "Kriege, N.M., Johansson, F.D., Morris, C.: A survey on graph kernels. Appl. Netw. Sci. 5(1), 6 (2020). https://doi.org/10.1007/s41109-019-0195-3\n",
    "\n",
    "Neumann, M., Garnett, R., Bauckhage, C., Kersting, K.: Propagation kernels: efficient graph kernels from propagated information. Machine Learning pp. 1–37 (2015), http://dx.doi.org/10.1007/ s10994-015-5517-9\n",
    "\n",
    "Winn, J. M., Criminisi, A., & Minka, T. P. (2005). Object categorization by learned universal visual dictionary. In 10th IEEE international conference on computer vision (ICCV-05), pp. 1800–1807.\n",
    "\n",
    "https://www.microsoft.com/en-us/research/project/image-understanding/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fobjectclassrecognition%2F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb85b6e",
   "metadata": {},
   "source": [
    "# GNN baseline\n",
    "\n",
    "The Tudataset contains baseline implementations of Graph Neural Networks for benchmark comparisons. We ran the GIN network with jumping knowledge to get an idea what kind of accuracy is achievable in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "eb1f6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import auxiliarymethods.datasets as dp\n",
    "from auxiliarymethods.gnn_evaluation import gnn_evaluation\n",
    "from gnn_baselines.gnn_architectures import GIN, GINE, GINEWithJK, GINWithJK\n",
    "\n",
    "\n",
    "def gnn_mrsc21():\n",
    "    num_reps = 1\n",
    "\n",
    "    ### Smaller datasets.\n",
    "    dataset = [[\"MSRC_21\", True]]\n",
    "\n",
    "    results = []\n",
    "    for d, use_labels in dataset:\n",
    "        # Download dataset.\n",
    "        dp.get_dataset(d)\n",
    "\n",
    "        # GIN, dataset d, layers in [1:6], hidden dimension in {32,64,128}.\n",
    "        #acc, s_1, s_2 = gnn_evaluation(GIN, d, [1, 2, 3, 4, 5], [32, 64, 128], max_num_epochs=200, batch_size=64,\n",
    "         #                              start_lr=0.01, num_repetitions=num_reps, all_std=True)\n",
    "        #print(d + \" \" + \"GIN \" + str(acc) + \" \" + str(s_1) + \" \" + str(s_2))\n",
    "        #results.append(d + \" \" + \"GIN \" + str(acc) + \" \" + str(s_1) + \" \" + str(s_2))\n",
    "\n",
    "        # GIN with jumping knowledge, dataset d, layers in [1:6], hidden dimension in {32,64,128}.\n",
    "        acc, s_1, s_2 = gnn_evaluation(GINWithJK, d, [1, 2, 3, 4, 5], [32, 64, 128], max_num_epochs=200,\n",
    "                                       batch_size=64,\n",
    "                                       start_lr=0.01, num_repetitions=num_reps, all_std=True)\n",
    "        print(d + \" \" + \"GINWithJK \" + str(acc) + \" \" + str(s_1) + \" \" + str(s_2))\n",
    "        results.append(d + \" \" + \"GINWithJK \" + str(acc) + \" \" + str(s_1) + \" \" + str(s_2))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02674dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dm_expl/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "gnn_mrsc21()\n",
    "# takes forever to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea435a3",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "## node2vec\n",
    "\n",
    "The node2vec embedding utilises 2nd order random walks of a given length to generate the network neighborhood of a node [1]. The bias in the walks is tuned by two hyperparameters p and q. The former serves as the probability to return to the previous node, while the latter gives the probability of moving further away from it. The number of walks, walk length and number of embedding dimensions can be specified. While the number of walks and their length increase the richness of the node embedding, they also severely affect performance [1]. Due to the generally high run-time, it was not possible to tune the hyperparameters to optimise them against our dataset. We simply used the default parameters suggested by the authors. \n",
    "\n",
    "The algorithm will return an embedding vector of a given dimensionality for each node. To relate these back to the overall graph, we used the approach of computing the sum, average and/or standard deviation of these embeddings for each graph.\n",
    "\n",
    "The Python implementation of the original authors was used [2], with minor changes to make it comptabile to the newest version of Gensim.\n",
    "\n",
    "[1] Grover, A., Leskovec, J.: Node2vec: Scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Min- ing. p. 855–864. KDD ’16, Association for Computing Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2939672.2939754, https://doi.org/10.1145/2939672.2939754\n",
    "\n",
    "[2] https://snap.stanford.edu/node2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1033ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "?node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3055e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node2vec(G, embedding_dims = 64, walk_length = 30, num_walks = 200, p = 1, q = 1):\n",
    "    G_node2vec = []\n",
    "    embedding_dims = 64\n",
    "    walk_length = 30\n",
    "    num_walks = 200\n",
    "\n",
    "    for i,graph in enumerate(G):\n",
    "\n",
    "        wv_filename = os.path.join(\"word_vectors\",\"vectors{}_wl{}_dims{}.kv\".format(i,walk_length,dims))\n",
    "        if not os.path.exists(wv_filename):\n",
    "            node2vec = Node2Vec(graph, dimensions=embedding_dims, walk_length=walk_length, num_walks=num_walks, workers=4)  # Use temp_folder for big graphs\n",
    "\n",
    "        # Embed nodes\n",
    "            model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "            model.wv.save(wv_filename)\n",
    "            G_node2vec.append(model.wv.get_normed_vectors())\n",
    "        else:\n",
    "            loaded_wv = KeyedVectors.load(wv_filename,mmap='r')\n",
    "            G_node2vec.append(loaded_wv.get_normed_vectors())\n",
    "    return G_node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_node2vec\n",
    "embedding_dims = 64\n",
    "G_node2vec = generate_node2vec(G,128,60,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3766a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de4668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_embeddings = {}\n",
    "\n",
    "node2vec_sums = np.zeros((len(G),embedding_dims))\n",
    "for i,embedding in enumerate(G_node2vec):\n",
    "    node2vec_sums[i] = embedding.sum(axis=0)\n",
    "node2vec_embeddings[\"sum\"] = node2vec_sums \n",
    "\n",
    "node2vec_means = np.zeros((len(G),embedding_dims))\n",
    "for i,embedding in enumerate(G_node2vec):\n",
    "    node2vec_means[i] = embedding.mean(axis=0)\n",
    "node2vec_embeddings[\"mean\"] = node2vec_means\n",
    "\n",
    "node2vec_sd = np.zeros((len(G),embedding_dims))\n",
    "for i,embedding in enumerate(G_node2vec):\n",
    "    node2vec_sd[i] = embedding.std(axis=0)\n",
    "node2vec_embeddings[\"sd\"] = node2vec_sd\n",
    "node2vec_embeddings[\"sum_mean_sd\"] = np.hstack([node2vec_sums, node2vec_means,node2vec_sd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98a8aa",
   "metadata": {},
   "source": [
    "## SpectralMix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255699fd",
   "metadata": {},
   "source": [
    "Objective function: \n",
    "\n",
    "$min_{O,M} \\sum_{r=1}^{|R|} \\alpha_{r} \\cdot \\left( \\sum_{e=(v_{i},v_{j},w_{ij},r) \\in E_{r}} w_{ij} \\cdot \\sum_{l=1}^{d} ||o_{il}-i_{jl}||^{2} \\right) + \\sum_{i=1}^{n} \\sum_{j=1}^{|A|} \\alpha_{j} \\cdot \\sum_{l=1}^{d}||o_{il}-m_{il}||^2$\n",
    "\n",
    "O = n x d matrix of coordinates with each row coordinate vector for node\n",
    "First part of equation: relational part of data, $|R|$dimensional graph with position of ata object (all edges between $v_{i}$ and $v_{j}$, coordinate matrix O minimises squared euclidean distances\n",
    "$o_{il}$ = coordinate of node $v_{i}$ in dimension l\n",
    "This happens over multiple relations\n",
    "d = dimension of embedding\n",
    "\n",
    "Second summand: categorial information as a bipartite graph between nodes and categories, one for each categorical variable\n",
    "c categorical graphs \n",
    "as in homogenity analysis, discover for every category $k_{j}$ of every attribute $A_{j}$ its position in $\\mathbb{R}^{d}$\n",
    "M (C x d matrix) contains coordinates of object $v_{i}$ in dimension l at position $m_{il}$\n",
    "\n",
    "$\\alpha_{r}, \\alpha_{j}$ = weighting factors for relation types and categorical attributes (suggested to be equal)\n",
    "\n",
    "constraint: orthonomality of O columns to ensure non-trivial solutions\n",
    "\n",
    "initialised with O random values, then update O and M to minimise, converge if no longer increases\n",
    "\n",
    "[1] Sadikaj, Y., Velaj, Y., Behzadi, S., Plant, C.: Spectral Clustering of Attributed Multi-Relational Graphs, p. 1431–1440. Association for Computing Machinery, New York, NY, USA (2021), https://doi.org/ 10.1145/3447548.3467381\n",
    "\n",
    "[2] https://gitlab.cs.univie.ac.at/yllis19cs/spectralmixpublic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4a9ae",
   "metadata": {},
   "source": [
    "## Clustering and classification algorithms\n",
    "\n",
    "The main task of this analysis is to study the effect of embeddings and graph properties on the algorithms, so we chose two very simple algorithms: Kmeans for clustering with a given cluster number of 20, and a SVM with a RBF kernel. The SVM uses a 80/20 training/test split and scaled data.\n",
    "\n",
    "We also experimented with using very simple neural networks for the classification task, but found that we could not achieve good results - possibly due to the small sample size of only ca. 580 images we have at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0d202989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "\n",
    "def kmeans_batch(data,labels,runs):\n",
    "    nmis = []\n",
    "    for i in range(0,runs):\n",
    "        kmeans = KMeans(n_clusters=20).fit(data)\n",
    "        nmis.append(normalized_mutual_info_score(labels,kmeans.labels_))\n",
    "    return np.mean(nmis)\n",
    "\n",
    "def simple_nn(X,Y,train_split = 0.7):\n",
    "    # shuffle data and split into train / test\n",
    "    shuffled = np.arange(len(X))\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X = X[shuffled]\n",
    "    Y = Y[shuffled]\n",
    "\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    n_train = int(train_split*len(X))\n",
    "    X_train= X[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    Y_train = Y[:n_train]\n",
    "    Y_test = Y[n_train:]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        #tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(64, activation='relu',input_shape=X_train.shape[1:]),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(np.unique(classes)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, to_categorical(Y_train-1), epochs=10)\n",
    "    test_loss, test_acc = model.evaluate(X_test,  to_categorical(Y_test-1), verbose=2)\n",
    "    \n",
    "    print('\\nTest accuracy:', test_acc)\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "def svm(X,Y,train_split = 0.8, runs = 10, kernel=\"rbf\"):\n",
    "    accuracies = []\n",
    "    for i in range(0,runs):\n",
    "    \n",
    "        shuffled = np.arange(len(X))\n",
    "        np.random.shuffle(shuffled)\n",
    "\n",
    "        X = X[shuffled]\n",
    "        Y = Y[shuffled]\n",
    "\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        n_train = int(train_split*len(X))\n",
    "        X_train= X[:n_train]\n",
    "        X_test = X[n_train:]\n",
    "        Y_train = Y[:n_train]\n",
    "        Y_test = Y[n_train:]\n",
    "\n",
    "        svm = sklearn.svm.SVC(kernel=kernel)\n",
    "        svm.fit(X_train,Y_train)\n",
    "\n",
    "        acc = svm.score(X_test,Y_test)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaa357",
   "metadata": {},
   "source": [
    "## Benchmarks for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a13b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_name, embedding in node2vec_embeddings.items():\n",
    "    nmi = svm(embedding,classes,kernel = \"rbf\",runs=20)\n",
    "    print(\"embedding: {} svm nmi: {}\".format(embedding_name,nmi))\n",
    "for embedding_name, embedding in node2vec_embeddings.items():\n",
    "    best_nmi = kmeans_batch(embedding,classes,runs=20)\n",
    "    print(\"embedding: {} average kmeans nmi: {}\".format(embedding_name,best_nmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2698070",
   "metadata": {},
   "source": [
    "# Enhancing the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33cd3b",
   "metadata": {},
   "source": [
    "# Node-level attributes\n",
    "\n",
    "Give those to SpectralMix, can deal with attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1688468",
   "metadata": {},
   "source": [
    "### Degree\n",
    "measures number of connected nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba55693",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    degrees = graph.degree()\n",
    "    for node_degree in degrees:\n",
    "        graph.nodes[node_degree[0]][\"degree\"] = node_degree[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G[0].nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5a2e5",
   "metadata": {},
   "source": [
    "### Eigenvector centrality\n",
    "Nodes are important if their neighbours are important. \n",
    "Eigenvector centrality of a node v $c_{v} = \\frac{1}{\\lambda}\\sum_{u \\in N(v)}c_{u}$\n",
    "\n",
    "High values for nodes that are tightly clustered together. Usefulness for image data: probably not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a30122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    node_centrality = nx.eigenvector_centrality(graph, max_iter = 1000)\n",
    "    nx.set_node_attributes(graph, eigenvector_centrality, \"eigenvector_centrality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638de73",
   "metadata": {},
   "source": [
    "### Betweenness centrality\n",
    "\n",
    "Nodes are important if they lie on many shortest path within graph. \n",
    "\n",
    "$c_{v} = \\sum_{s \\neq v \\neq t}\\frac{\\text{# shortest path between s and t that contains v}}{\\text{# shortest path between s and t}}$\n",
    "\n",
    "The shortest paths can be weighted by edge attributes greater than zero, such that the weight is interpreted as distance between node. Further, the results can be normalized.\n",
    "\n",
    "High values indicate that a node serves an important connecting function in the overall graph. Usefulness for our image data: probably not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    betweenness_centrality = nx.betweenness_centrality(graph, normalized=True)\n",
    "    nx.set_node_attributes(graph, betweenness_centrality, \"betweenness_centrality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acda1c",
   "metadata": {},
   "source": [
    "### Closeness centrality\n",
    "\n",
    "Importance is measured if length of shortest path to all other nodes is small\n",
    "\n",
    "$c_{v} = \\frac{1}{\\sum_{u \\neq v}\\text{shortest path length between u and v}}$\n",
    "\n",
    "In Networkx, this measure goes under harmonic centrality, while closeness centrality is implemented as \n",
    "\n",
    "$C(u)=\\frac{n-1}{\\sum_{v=1}^{n-1}d(v,u)}$ \n",
    "\n",
    "where n-1 nodes are reachable from v and d(v,u) is the shortest path distance between v and u. If the graphs consist only of connected components, as in our case, the measure should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe8da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "    nx.set_node_attributes(graph, closeness_centrality, \"closeness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    harmonic_centrality = nx.harmonic_centrality(graph)\n",
    "    nx.set_node_attributes(graph, harmonic_centrality, \"harmonic_centrality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c978183a",
   "metadata": {},
   "source": [
    "### Clustering coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d778ebf",
   "metadata": {},
   "source": [
    "Nodes are important if their neighbouring nodes are connected with each other.\n",
    "\n",
    "$e_{v} = \\frac{\\text{#edges among neighbouring nodes}}{(\\frac{k_{v}}{2})} \\in [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7627190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    clustering_coefficient = nx.clustering(graph)\n",
    "    nx.set_node_attributes(graph, clustering_coefficient, \"clustering_coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d128330",
   "metadata": {},
   "source": [
    "### Pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a9834",
   "metadata": {},
   "source": [
    "Recursive definition:\n",
    "\n",
    "$PR_{i}=\\frac{1-d}{n}+d\\sum_{j=1}^{n}\\frac{PR_{j}}{c_{j}}$\n",
    "\n",
    "PageRank of node i is the Pagerank of all nodes j linking to it, divided by the number of nodes c that j links to.\n",
    "It is weighted by a dampening factor d (0-1) that prevents nodes without outgoing links to suck up all the weight, a part of the total sum is distributed among all nodes n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "66483a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    pagerank = nx.pagerank(graph)\n",
    "    nx.set_node_attributes(graph, pagerank, \"pagerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree sequence by node label?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678bedd",
   "metadata": {},
   "source": [
    "# Graph-attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711f724",
   "metadata": {},
   "source": [
    "### Number of nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b6bc806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    no_nodes = len(graph.nodes)\n",
    "    no_edges = len(graph.edges)\n",
    "    graph.graph[\"nodes\"] = no_nodes\n",
    "    graph.graph[\"edges\"] = no_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bd897",
   "metadata": {},
   "source": [
    "### Degree histogram or distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bc0762e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequences = []\n",
    "dmax_total = 0\n",
    "for graph in G:\n",
    "    # after networkx doc\n",
    "    degree_sequence = sorted((d for n, d in graph.degree()), reverse=True)\n",
    "    dmax = max(degree_sequence)\n",
    "    if dmax > dmax_total:\n",
    "        dmax_total = dmax\n",
    "    unique, counts = np.unique(degree_sequence, return_counts=True)\n",
    "    \n",
    "    degree_sequence = np.zeros((dmax+1))\n",
    "    for label,count in zip(unique,counts):\n",
    "        degree_sequence[label] = count\n",
    "    degree_sequences.append(degree_sequence)\n",
    "    \n",
    "for graph,sequence in zip(G,degree_sequences):\n",
    "    degree_hist = np.zeros((dmax_total+1))\n",
    "    degree_hist[:len(sequence)] = sequence\n",
    "    graph.graph[\"degree_hist\"] = degree_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50197295",
   "metadata": {},
   "source": [
    "### Node label histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "24474b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_sequences = []\n",
    "nlmax_total = 0\n",
    "for graph in G:\n",
    "\n",
    "    nl_max = 0\n",
    "    for node in graph.nodes.items():\n",
    "        label = node[1][\"labels\"][0]\n",
    "        if label > nl_max:\n",
    "            nl_max = label\n",
    "    nl_sequence = np.zeros((nl_max+1))\n",
    "    \n",
    "    for node in graph.nodes.items():\n",
    "        label = node[1][\"labels\"][0]\n",
    "        nl_sequence[label] += 1\n",
    "    \n",
    "    if nl_max > nlmax_total:\n",
    "        nlmax_total = nl_max\n",
    "    nl_sequences.append(nl_sequence)\n",
    "\n",
    "for graph,sequence in zip(G,nl_sequences):\n",
    "    nl_hist = np.zeros((nlmax_total+1))\n",
    "    nl_hist[:len(sequence)] = sequence\n",
    "    graph.graph[\"node_label_hist\"] = nl_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b706f",
   "metadata": {},
   "source": [
    "### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a5c0f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of nodes you need to remove to disconnect graph\n",
    "\n",
    "for graph in G:\n",
    "    connectivity = nx.node_connectivity(graph)\n",
    "    graph.graph[\"connectivity\"] = connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9190f3",
   "metadata": {},
   "source": [
    "### Max clique labels\n",
    "\n",
    "Cliques are the subgraphs of any given graph that are complete. Corresponding to our image data, these might be considered as areas of densely packed information. By including the node labels for these, we hope to emphasize areas of high information in the picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "81efdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    clique_lists = list(nx.find_cliques(graph))\n",
    "    clique_label_hist = np.zeros_like(graph.graph[\"node_label_hist\"])\n",
    "    for clique_list in clique_lists:\n",
    "        for node in clique_list:\n",
    "            label = graph.nodes[node][\"labels\"][0]\n",
    "            clique_label_hist[label] += 1\n",
    "    graph.graph[\"clique_label_hist\"] = clique_label_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945892db",
   "metadata": {},
   "source": [
    "### Treewidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9dc4c",
   "metadata": {},
   "source": [
    "How close graph is to a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3d40b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in G:\n",
    "    tree_width = nx.approximation.treewidth_min_degree(graph)\n",
    "    graph.graph[\"tree_width\"] = tree_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddb617",
   "metadata": {},
   "source": [
    "## node2vec enhanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9c7258ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sum', 'mean', 'sd', 'sum_mean_sd'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node2vec_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4d4ce242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add best performing base embedding\n",
    "# add graph level attributes\n",
    "# see what performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f6360b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label_hist = np.zeros((len(G),25))\n",
    "degree_hist = np.zeros((len(G),24))\n",
    "tree_widths = np.zeros((len(G),1))\n",
    "connectivities = np.zeros((len(G),1))\n",
    "clique_hist = np.zeros((len(G),25))\n",
    "no_nodes = np.zeros((len(G),1))\n",
    "no_edges = np.zeros((len(G),1))\n",
    "pagerank_hist = np.zeros((len(G),20))\n",
    "\n",
    "for i,graph in enumerate(G):\n",
    "    node_label_hist[i,:] = graph.graph[\"node_label_hist\"]\n",
    "    degree_hist[i,:] = graph.graph[\"degree_hist\"]\n",
    "    tree_widths[i,:] = graph.graph[\"tree_width\"][0]\n",
    "    connectivities[i,:] = graph.graph[\"connectivity\"]\n",
    "    clique_hist[i,:] = graph.graph[\"clique_label_hist\"]\n",
    "    no_nodes[i,:] = graph.graph[\"nodes\"]\n",
    "    no_edges[i,:] = graph.graph[\"edges\"]\n",
    "    pagerank_hist[i,:] = np.histogram(list(nx.get_node_attributes(graph,\"pagerank\").values()),20)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f7471",
   "metadata": {},
   "source": [
    "## Benchmarks of enhanced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "ebe1a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All graph properties without node label information\n",
      "Combination of properties\n",
      "Node label information only\n"
     ]
    }
   ],
   "source": [
    "node2vec_enhanced = {}\n",
    "\n",
    "#All graph properties without node label information\n",
    "node2vec_enhanced[\"sum_all_without_node_labels\"] = np.hstack([node2vec_embeddings[\"sum\"],pagerank_hist,no_nodes,no_edges,tree_widths,connectivities,degree_hist])\n",
    "\n",
    "#Combination of properties\n",
    "node2vec_enhanced[\"sum_label_hist\"] = np.hstack([node2vec_embeddings[\"sum\"],node_label_hist])\n",
    "node2vec_enhanced[\"sum_label_degree_hist\"] = np.hstack([node2vec_embeddings[\"sum\"],node_label_hist,degree_hist])\n",
    "node2vec_enhanced[\"sum_label_hist_tree_width\"] = np.hstack([node2vec_embeddings[\"sum\"],node_label_hist,tree_widths])\n",
    "node2vec_enhanced[\"sum_all\"] = np.hstack([node2vec_embeddings[\"sum\"],no_nodes,no_edges,clique_hist,node_label_hist,tree_widths,connectivities,degree_hist,pagerank_hist])\n",
    "\n",
    "# Node label information only\n",
    "node2vec_enhanced[\"sum_clique_label\"] = np.hstack([node2vec_embeddings[\"sum\"],clique_hist])\n",
    "node2vec_enhanced[\"mean_labels\"] = np.hstack([node2vec_embeddings[\"mean\"],clique_hist,node_label_hist])\n",
    "node2vec_enhanced[\"sum_labels\"] = np.hstack([node2vec_embeddings[\"sum\"],clique_hist,node_label_hist])\n",
    "node2vec_enhanced[\"sum_mean_sd_labels\"] = np.hstack([node2vec_embeddings[\"sum_mean_sd\"],clique_hist,node_label_hist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1cc2f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: sum_all_without_node_labels average kmeans nmi: 0.1520658976494376\n",
      "embedding: sum_label_hist average kmeans nmi: 0.6114861924203832\n",
      "embedding: sum_label_degree_hist average kmeans nmi: 0.6065191172340347\n",
      "embedding: sum_label_hist_tree_width average kmeans nmi: 0.6123510474665526\n",
      "embedding: sum_all average kmeans nmi: 0.6371894973981337\n",
      "embedding: sum_clique_label average kmeans nmi: 0.6462349392226977\n",
      "embedding: mean_labels average kmeans nmi: 0.6480020967956747\n",
      "embedding: sum_labels average kmeans nmi: 0.6496618516286274\n",
      "embedding: sum_mean_sd_labels average kmeans nmi: 0.6456424046293121\n"
     ]
    }
   ],
   "source": [
    "for embedding_name, embedding in node2vec_enhanced.items():\n",
    "    best_nmi = kmeans_batch(embedding,classes,20)\n",
    "    print(\"embedding: {} average kmeans nmi: {}\".format(embedding_name,best_nmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "62592f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: sum_all_without_node_labels average svm nmi: 0.0801775147928994\n",
      "embedding: sum_label_hist average svm nmi: 0.7242603550295857\n",
      "embedding: sum_label_degree_hist average svm nmi: 0.7032544378698226\n",
      "embedding: sum_label_hist_tree_width average svm nmi: 0.7369822485207101\n",
      "embedding: sum_all average svm nmi: 0.7863905325443787\n",
      "embedding: sum_clique_label average svm nmi: 0.7331360946745563\n",
      "embedding: mean_labels average svm nmi: 0.8076923076923077\n",
      "embedding: sum_labels average svm nmi: 0.7920118343195267\n",
      "embedding: sum_mean_sd_labels average svm nmi: 0.6926035502958579\n"
     ]
    }
   ],
   "source": [
    "# have to repeat and average, varies wildly\n",
    "for embedding_name, embedding in node2vec_enhanced.items():\n",
    "    nmi = svm(embedding,classes,runs = 20, kernel = \"rbf\")\n",
    "    print(\"embedding: {} average svm nmi: {}\".format(embedding_name,nmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59ddfe",
   "metadata": {},
   "source": [
    "# Discussion of results\n",
    "\n",
    "### node2vec\n",
    "Adding any graph structural properties only marginally increases results for both classifiers. The by far biggest impact can be achieved by adding the node label histogram and the clique nodel label histogram. Both combined with the mean aggregation of node2vec can achieve average accuracies over 0.8 for the SVM. They lose accuracy if they are diluted with any number of graph properties that are not related to node label information.\n",
    "\n",
    "Overall, it seems that node2vec already covers most information that can be gained from graph properties. It only lacks the ability to directly incorporate label information, which can be amended by simple concatenation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ce32d",
   "metadata": {},
   "source": [
    "### SpectralMix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8995bc",
   "metadata": {},
   "source": [
    "resources used\n",
    "GNN: \n",
    "https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b\n",
    "https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n",
    "https://distill.pub/2021/gnn-intro/#graph-to-tensor\n",
    "https://antonsruberts.github.io/graph/gcn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e813355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some graphs with labels as info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
